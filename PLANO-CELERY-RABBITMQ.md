# ğŸ“‹ Plano de ImplementaÃ§Ã£o: Sistema de Filas AssÃ­ncronas com Celery + RabbitMQ

## ğŸ“Œ Contexto

O Odoo possui apenas `ir.cron` nativo (scheduled actions) que executa tarefas de forma **sÃ­ncrona dentro dos workers do Odoo**, o que pode sobrecarregar a aplicaÃ§Ã£o. Para processamento assÃ­ncrono real e escalÃ¡vel, Ã© necessÃ¡rio um sistema de filas externo.

### Problema Atual
- âŒ `ir.cron` executa no mesmo processo do Odoo
- âŒ Tarefas pesadas bloqueiam workers
- âŒ Sem paralelizaÃ§Ã£o real
- âŒ NÃ£o escala horizontalmente
- âŒ Polling no banco a cada 60 segundos

### SoluÃ§Ã£o Proposta: **Celery + RabbitMQ**

**Por quÃª?**
- âœ… Desacoplado do Odoo (workers em processos separados)
- âœ… Industry standard para Python
- âœ… EscalÃ¡vel horizontalmente
- âœ… Monitoramento via Flower
- âœ… Retry automÃ¡tico e controle de prioridades
- âœ… Usa Redis (jÃ¡ configurado no projeto)

---

## ğŸ“ Arquitetura Proposta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    envia    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   entrega   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Odoo     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚   RabbitMQ   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚   Celery    â”‚
â”‚  (Producer) â”‚   tarefa    â”‚  (Message    â”‚   tarefa    â”‚   Worker    â”‚
â”‚             â”‚             â”‚   Broker)    â”‚             â”‚ (Executor)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â†‘                            â”‚
                                   â”‚       resultado            â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         (via Redis)
```

### Componentes

| Componente | Responsabilidade | Tecnologia |
|------------|------------------|------------|
| **Odoo (Producer)** | Enfileira tarefas | Python + Celery client |
| **RabbitMQ (Broker)** | Gerencia fila de mensagens | RabbitMQ 3.x |
| **Redis (Backend)** | Armazena resultados | Redis 7.x (jÃ¡ configurado) |
| **Celery Worker** | Executa tarefas | Python + Celery |
| **Flower** | Monitoramento | Flower dashboard |

---

## ğŸ—‚ï¸ Estrutura de Arquivos

```
18.0/
â”œâ”€â”€ docker-compose.yml              # Adicionar RabbitMQ, Celery Worker, Flower
â”œâ”€â”€ .env                            # Adicionar secrets RabbitMQ/Flower
â”œâ”€â”€ extra-addons/
â”‚   â””â”€â”€ thedevkitchen_celery/       # ğŸ†• MÃ³dulo Odoo
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ __manifest__.py
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ celery_task.py      # Model para gerenciar tasks
â”‚       â”œâ”€â”€ views/
â”‚       â”‚   â”œâ”€â”€ celery_task_views.xml
â”‚       â”‚   â””â”€â”€ celery_task_menu.xml
â”‚       â”œâ”€â”€ security/
â”‚       â”‚   â”œâ”€â”€ ir.model.access.csv
â”‚       â”‚   â””â”€â”€ celery_security.xml
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â””â”€â”€ ir_cron_data.xml    # Cron para sincronizar status
â”‚       â””â”€â”€ celery_client/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ client.py            # Cliente Celery (envia tasks)
â””â”€â”€ celery_worker/                   # ğŸ†• Worker Celery (FORA do Odoo)
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ requirements.txt             # celery, pika, pandas, etc.
    â”œâ”€â”€ tasks.py                     # DefiniÃ§Ãµes das tasks
    â”œâ”€â”€ odoo_connector.py            # XML-RPC para acessar Odoo
    â”œâ”€â”€ config.py                    # ConfiguraÃ§Ãµes
    â””â”€â”€ README.md
```

---

## ğŸ“ Fases de ImplementaÃ§Ã£o

### **Fase 1: Infraestrutura Base (4-6 horas)**

**Objetivo:** Configurar RabbitMQ e Celery Worker no Docker

#### Tarefas

- [ ] **1.1. Adicionar RabbitMQ ao docker-compose.yml**
  ```yaml
  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: rabbitmq
    ports:
      - "5672:5672"   # AMQP
      - "15672:15672" # Management UI
    environment:
      RABBITMQ_DEFAULT_USER: odoo
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
    volumes:
      - rabbitmq-data:/var/lib/rabbitmq
    networks:
      - odoo-net
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
  ```

- [ ] **1.2. Criar diretÃ³rio e Dockerfile do Celery Worker**
  ```dockerfile
  # celery_worker/Dockerfile
  FROM python:3.11-slim
  
  WORKDIR /app
  
  COPY requirements.txt .
  RUN pip install --no-cache-dir -r requirements.txt
  
  COPY . .
  
  CMD ["celery", "-A", "tasks", "worker", "--loglevel=info", "--concurrency=4"]
  ```

- [ ] **1.3. Criar requirements.txt do Celery Worker**
  ```txt
  celery[redis]==5.3.4
  pika==1.3.2
  pandas==2.1.4
  requests==2.31.0
  python-dotenv==1.0.0
  ```

- [ ] **1.4. Adicionar Celery Worker ao docker-compose.yml**
  ```yaml
  celery_worker:
    build: ./celery_worker
    container_name: celery_worker
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      odoo:
        condition: service_started
    environment:
      CELERY_BROKER_URL: amqp://odoo:${RABBITMQ_PASSWORD}@rabbitmq:5672//
      CELERY_RESULT_BACKEND: redis://redis:6379/2
      ODOO_URL: http://odoo:8069
      ODOO_DB: realestate
      ODOO_USER: admin
      ODOO_PASSWORD: ${ODOO_ADMIN_PASSWORD}
    networks:
      - odoo-net
    restart: unless-stopped
  ```

- [ ] **1.5. Adicionar Flower (monitoramento) ao docker-compose.yml**
  ```yaml
  flower:
    image: mher/flower:2.0
    container_name: flower
    ports:
      - "5555:5555"
    environment:
      CELERY_BROKER_URL: amqp://odoo:${RABBITMQ_PASSWORD}@rabbitmq:5672//
      CELERY_RESULT_BACKEND: redis://redis:6379/2
      FLOWER_BASIC_AUTH: admin:${FLOWER_PASSWORD}
    depends_on:
      - rabbitmq
    networks:
      - odoo-net
  ```

- [ ] **1.6. Adicionar volume do RabbitMQ**
  ```yaml
  volumes:
    odoo18-db:
    odoo18-data:
    odoo18-redis:
    rabbitmq-data:  # ğŸ†•
  ```

- [ ] **1.7. Atualizar .env com secrets**
  ```env
  RABBITMQ_PASSWORD=strong_password_here
  FLOWER_PASSWORD=strong_password_here
  ODOO_ADMIN_PASSWORD=admin
  ```

- [ ] **1.8. Testar infraestrutura**
  ```bash
  docker compose up -d rabbitmq
  docker compose logs rabbitmq
  # Acessar: http://localhost:15672 (user: odoo, pass: RABBITMQ_PASSWORD)
  ```

**Arquivos modificados:**
- `18.0/docker-compose.yml`
- `18.0/.env`

**Arquivos criados:**
- `18.0/celery_worker/Dockerfile`
- `18.0/celery_worker/requirements.txt`

---

### **Fase 2: MÃ³dulo Odoo - Cliente Celery (6-8 horas)**

**Objetivo:** Criar mÃ³dulo Odoo para enfileirar tarefas

#### Tarefas

- [ ] **2.1. Criar estrutura do mÃ³dulo `thedevkitchen_celery`**
  ```python
  # __manifest__.py
  {
      'name': 'Celery Integration',
      'version': '18.0.1.0.0',
      'category': 'Technical',
      'summary': 'Asynchronous task queue with Celery + RabbitMQ',
      'depends': ['base'],
      'data': [
          'security/celery_security.xml',
          'security/ir.model.access.csv',
          'data/ir_cron_data.xml',
          'views/celery_task_views.xml',
          'views/celery_task_menu.xml',
      ],
      'installable': True,
      'application': False,
  }
  ```

- [ ] **2.2. Criar cliente Celery (`celery_client/client.py`)**
  ```python
  from celery import Celery
  import os
  
  celery_app = Celery(
      'odoo_tasks',
      broker=os.getenv('CELERY_BROKER_URL', 'amqp://odoo:odoo@rabbitmq:5672//'),
      backend=os.getenv('CELERY_RESULT_BACKEND', 'redis://redis:6379/2')
  )
  
  class Tasks:
      @staticmethod
      def enviar_email_lote(record_ids, subject, body):
          return celery_app.send_task(
              'tasks.enviar_email_lote',
              args=[record_ids, subject, body]
          )
      
      @staticmethod
      def processar_importacao(file_path, model_name):
          return celery_app.send_task(
              'tasks.processar_importacao',
              args=[file_path, model_name]
          )
  
  tasks = Tasks()
  ```

- [ ] **2.3. Criar model `celery.task.queue`**
  ```python
  from odoo import models, fields, api
  from ..celery_client.client import tasks, celery_app
  
  class CeleryTaskQueue(models.Model):
      _name = 'celery.task.queue'
      _description = 'Celery Task Queue Manager'
      _order = 'create_date desc'
      
      task_id = fields.Char('Task ID', readonly=True, index=True)
      task_name = fields.Char('Task Name', required=True)
      state = fields.Selection([
          ('pending', 'Pending'),
          ('running', 'Running'),
          ('success', 'Success'),
          ('failed', 'Failed'),
          ('retry', 'Retrying')
      ], default='pending', string='Status')
      result = fields.Text('Result', readonly=True)
      error_message = fields.Text('Error Message', readonly=True)
      args = fields.Text('Arguments')
      progress = fields.Float('Progress (%)', default=0.0)
      
      def action_check_status(self):
          """Consulta status da task no Celery"""
          self.ensure_one()
          if self.task_id:
              result = celery_app.AsyncResult(self.task_id)
              self.state = result.state.lower()
              if result.successful():
                  self.result = str(result.result)
              elif result.failed():
                  self.error_message = str(result.info)
      
      @api.model
      def cron_update_tasks_status(self):
          """Cron job para atualizar status de tasks pendentes"""
          tasks = self.search([('state', 'in', ['pending', 'running'])])
          tasks.action_check_status()
  ```

- [ ] **2.4. Criar views XML**
  ```xml
  <!-- views/celery_task_views.xml -->
  <record id="view_celery_task_tree" model="ir.ui.view">
      <field name="name">celery.task.queue.tree</field>
      <field name="model">celery.task.queue</field>
      <field name="arch" type="xml">
          <list>
              <field name="task_name"/>
              <field name="state" decoration-success="state == 'success'" 
                     decoration-danger="state == 'failed'"/>
              <field name="progress" widget="progressbar"/>
              <field name="create_date"/>
          </list>
      </field>
  </record>
  ```

- [ ] **2.5. Criar security files**
  - `security/celery_security.xml` (grupos)
  - `security/ir.model.access.csv` (permissÃµes)

- [ ] **2.6. Criar cron job para sync status**
  ```xml
  <!-- data/ir_cron_data.xml -->
  <record id="ir_cron_celery_task_status" model="ir.cron">
      <field name="name">Celery: Update Task Status</field>
      <field name="model_id" ref="model_celery_task_queue"/>
      <field name="state">code</field>
      <field name="code">model.cron_update_tasks_status()</field>
      <field name="interval_number">1</field>
      <field name="interval_type">minutes</field>
      <field name="numbercall">-1</field>
      <field name="active">True</field>
  </record>
  ```

- [ ] **2.7. Adicionar Celery ao Dockerfile do Odoo**
  ```dockerfile
  RUN pip3 install --break-system-packages celery[redis]==5.3.4
  ```

**Arquivos criados:**
- `18.0/extra-addons/thedevkitchen_celery/*` (estrutura completa)

**Arquivos modificados:**
- `18.0/Dockerfile`

---

### **Fase 3: Celery Worker - ImplementaÃ§Ã£o das Tasks (8-10 horas)**

**Objetivo:** Criar workers que executam tarefas em background

#### Tarefas

- [ ] **3.1. Criar `celery_worker/config.py`**
  ```python
  import os
  from dotenv import load_dotenv
  
  load_dotenv()
  
  CELERY_BROKER_URL = os.getenv('CELERY_BROKER_URL')
  CELERY_RESULT_BACKEND = os.getenv('CELERY_RESULT_BACKEND')
  ODOO_URL = os.getenv('ODOO_URL')
  ODOO_DB = os.getenv('ODOO_DB')
  ODOO_USER = os.getenv('ODOO_USER')
  ODOO_PASSWORD = os.getenv('ODOO_PASSWORD')
  ```

- [ ] **3.2. Criar `celery_worker/odoo_connector.py`**
  ```python
  import xmlrpc.client
  from config import ODOO_URL, ODOO_DB, ODOO_USER, ODOO_PASSWORD
  
  class OdooConnector:
      def __init__(self):
          self.url = ODOO_URL
          self.db = ODOO_DB
          self.username = ODOO_USER
          self.password = ODOO_PASSWORD
          
          # Autentica
          common = xmlrpc.client.ServerProxy(f'{self.url}/xmlrpc/2/common')
          self.uid = common.authenticate(self.db, self.username, self.password, {})
          
          # Client de execuÃ§Ã£o
          self.models = xmlrpc.client.ServerProxy(f'{self.url}/xmlrpc/2/object')
      
      def execute(self, model, method, *args, **kwargs):
          return self.models.execute_kw(
              self.db, self.uid, self.password,
              model, method, args, kwargs
          )
      
      def search_read(self, model, domain=[], fields=[]):
          return self.execute(model, 'search_read', domain, {'fields': fields})
      
      def create(self, model, values):
          return self.execute(model, 'create', [values])
      
      def write(self, model, ids, values):
          return self.execute(model, 'write', [ids, values])
  
  odoo = OdooConnector()
  ```

- [ ] **3.3. Criar `celery_worker/tasks.py` - Tasks bÃ¡sicas**
  ```python
  from celery import Celery
  from config import CELERY_BROKER_URL, CELERY_RESULT_BACKEND
  from odoo_connector import odoo
  import logging
  
  app = Celery('tasks', broker=CELERY_BROKER_URL, backend=CELERY_RESULT_BACKEND)
  
  logger = logging.getLogger(__name__)
  
  @app.task(name='tasks.enviar_email_lote', bind=True, max_retries=3)
  def enviar_email_lote(self, record_ids, subject, body):
      """Envia emails em lote sem bloquear Odoo"""
      try:
          for idx, record_id in enumerate(record_ids):
              # Busca dados do parceiro
              partner = odoo.execute('res.partner', 'read', [record_id], ['name', 'email'])[0]
              
              # Simula envio de email (substituir por SMTP real)
              logger.info(f"Enviando email para {partner['email']}")
              
              # Atualiza progresso
              progress = ((idx + 1) / len(record_ids)) * 100
              self.update_state(state='PROGRESS', meta={'progress': progress})
          
          return f"Enviados {len(record_ids)} emails com sucesso"
      
      except Exception as exc:
          logger.error(f"Erro ao enviar emails: {exc}")
          raise self.retry(exc=exc, countdown=60)
  
  @app.task(name='tasks.processar_importacao', bind=True)
  def processar_importacao(self, file_path, model_name):
      """Importa dados de CSV sem bloquear Odoo"""
      import pandas as pd
      
      try:
          df = pd.read_csv(file_path)
          total = len(df)
          
          for idx, row in df.iterrows():
              # Cria registro no Odoo
              odoo.create(model_name, row.to_dict())
              
              # Atualiza progresso
              progress = ((idx + 1) / total) * 100
              self.update_state(state='PROGRESS', meta={'progress': progress})
          
          return f"Importados {total} registros"
      
      except Exception as exc:
          logger.error(f"Erro na importaÃ§Ã£o: {exc}")
          raise
  
  @app.task(name='tasks.gerar_relatorio_pdf')
  def gerar_relatorio_pdf(report_name, record_ids):
      """Gera relatÃ³rio PDF pesado"""
      # ImplementaÃ§Ã£o futura
      pass
  
  @app.task(name='tasks.sincronizar_api_externa')
  def sincronizar_api_externa(endpoint, data):
      """Sincroniza com API externa"""
      # ImplementaÃ§Ã£o futura
      pass
  ```

- [ ] **3.4. Implementar retry e error handling**
  - Configurar retry automÃ¡tico
  - Exponential backoff
  - Dead letter queue

- [ ] **3.5. Adicionar logging estruturado**
  ```python
  import logging
  logging.basicConfig(
      level=logging.INFO,
      format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
  )
  ```

**Arquivos criados:**
- `celery_worker/config.py`
- `celery_worker/odoo_connector.py`
- `celery_worker/tasks.py`

---

### **Fase 4: IntegraÃ§Ã£o e Casos de Uso (6-8 horas)**

**Objetivo:** Integrar Celery em funcionalidades existentes

#### Use Cases

- [ ] **4.1. Use Case 1: Envio de Emails em Massa**
  - Adicionar botÃ£o em `res.partner`
  - MÃ©todo que enfileira task
  - NotificaÃ§Ã£o quando concluir

- [ ] **4.2. Use Case 2: ImportaÃ§Ã£o de ImÃ³veis CSV**
  - Upload CSV em `quicksol.property`
  - Processar em background
  - Barra de progresso

- [ ] **4.3. Use Case 3: GeraÃ§Ã£o de RelatÃ³rios**
  - RelatÃ³rio anual pesado
  - Processar assincronamente
  - Download quando pronto

- [ ] **4.4. Use Case 4: SincronizaÃ§Ã£o com APIs**
  - Sincronizar estoque
  - Retry automÃ¡tico
  - Logs de erro

**Exemplo de integraÃ§Ã£o:**
```python
# Em qualquer model do Odoo
from odoo.addons.thedevkitchen_celery.celery_client.client import tasks

def action_send_bulk_emails(self):
    # Enfileira task
    task = tasks.enviar_email_lote(
        record_ids=self.ids,
        subject="Bem-vindo!",
        body="OlÃ¡!"
    )
    
    # Registra na fila
    self.env['celery.task.queue'].create({
        'task_id': task.id,
        'task_name': 'enviar_email_lote',
        'args': str({'ids': self.ids})
    })
    
    return {
        'type': 'ir.actions.client',
        'tag': 'display_notification',
        'params': {
            'message': 'Emails sendo enviados em background',
            'type': 'success',
        }
    }
```

---

### **Fase 5: Monitoramento e Observabilidade (4-6 horas)**

**Objetivo:** Garantir visibilidade das tarefas

#### Tarefas

- [ ] **5.1. Configurar Flower Dashboard**
  - AcessÃ­vel em `http://localhost:5555`
  - AutenticaÃ§Ã£o bÃ¡sica
  - Visualizar tasks em tempo real

- [ ] **5.2. Implementar notificaÃ§Ãµes no Odoo**
  - Notificar quando task terminar
  - Email em caso de falha
  - Alerta no Chatter

- [ ] **5.3. Criar dashboard no Odoo**
  - GrÃ¡fico de tasks por status
  - Performance metrics
  - Taxa de sucesso/falha

- [ ] **5.4. Configurar alertas**
  - Alerta se fila > 1000 tasks
  - Alerta se worker offline
  - Email para admin

**Ferramentas:**
- Flower: http://localhost:5555
- RabbitMQ Management: http://localhost:15672

---

### **Fase 6: Testes e DocumentaÃ§Ã£o (8-10 horas)**

**Objetivo:** Garantir qualidade

#### Tarefas

- [ ] **6.1. Testes UnitÃ¡rios - MÃ³dulo Odoo**
  ```python
  def test_enqueue_task(self):
      task = self.env['celery.task.queue'].create({
          'task_name': 'test_task'
      })
      self.assertEqual(task.state, 'pending')
  ```

- [ ] **6.2. Testes UnitÃ¡rios - Worker**
  ```python
  from tasks import enviar_email_lote
  
  def test_enviar_email_lote():
      result = enviar_email_lote.apply([1, 2, 3], "Test", "Body")
      assert result.successful()
  ```

- [ ] **6.3. Testes E2E - Cypress**
  ```javascript
  it('Deve enfileirar task de email', () => {
      cy.visit('/web#model=res.partner')
      cy.get('[name="action_send_bulk_emails"]').click()
      cy.contains('Emails sendo enviados em background').should('be.visible')
  })
  ```

- [ ] **6.4. DocumentaÃ§Ã£o**
  - README.md do mÃ³dulo
  - README.md do worker
  - Diagrama de arquitetura
  - Guia de uso

- [ ] **6.5. ADR (Architecture Decision Record)**
  - `docs/adr/ADR-010-celery-rabbitmq-integration.md`
  - Justificativa tÃ©cnica
  - Trade-offs
  - Alternativas consideradas

**Arquivos criados:**
- `18.0/extra-addons/thedevkitchen_celery/tests/*`
- `18.0/extra-addons/thedevkitchen_celery/README.md`
- `celery_worker/README.md`
- `docs/adr/ADR-010-celery-rabbitmq-integration.md`

---

## ğŸ”§ Comandos Ãšteis

### Desenvolvimento
```bash
# Subir apenas RabbitMQ
docker compose up -d rabbitmq

# Ver logs do worker
docker compose logs -f celery_worker

# Acessar RabbitMQ Management
open http://localhost:15672

# Acessar Flower
open http://localhost:5555

# Testar task manualmente
docker compose exec celery_worker python -c "from tasks import enviar_email_lote; enviar_email_lote.delay([1,2,3], 'Test', 'Body')"
```

### Monitoramento
```bash
# Ver filas do RabbitMQ
docker compose exec rabbitmq rabbitmqctl list_queues

# Ver workers ativos (via Flower API)
curl http://admin:password@localhost:5555/api/workers

# Ver tasks pendentes
docker compose exec odoo odoo shell -d realestate
>>> env['celery.task.queue'].search([('state', '=', 'pending')])
```

### Troubleshooting
```bash
# Limpar fila do RabbitMQ
docker compose exec rabbitmq rabbitmqctl purge_queue celery

# Reiniciar worker
docker compose restart celery_worker

# Ver erros no Redis
docker compose exec redis redis-cli
> SELECT 2
> KEYS *
```

---

## ğŸ“Š MÃ©tricas de Sucesso

- [ ] Tasks executam em processo separado do Odoo
- [ ] Odoo responde instantaneamente ao enfileirar tasks
- [ ] Workers escalÃ¡veis (adicionar containers conforme carga)
- [ ] Monitoramento em tempo real via Flower
- [ ] Retry automÃ¡tico em caso de falha (max 3 tentativas)
- [ ] Performance: 100+ tasks/minuto por worker
- [ ] LatÃªncia: < 1 segundo para enfileirar
- [ ] Taxa de sucesso: > 95%

---

## ğŸš¨ Riscos e MitigaÃ§Ãµes

| Risco | Impacto | Probabilidade | MitigaÃ§Ã£o |
|-------|---------|---------------|-----------|
| Complexidade arquitetural | Alto | MÃ©dia | DocumentaÃ§Ã£o detalhada, ADR, diagramas |
| Debugging mais difÃ­cil | MÃ©dio | Alta | Logs estruturados, Flower dashboard, tracing |
| DependÃªncia de RabbitMQ | Alto | Baixa | Healthchecks, restart automÃ¡tico, alertas |
| SeguranÃ§a XML-RPC | MÃ©dio | MÃ©dia | UsuÃ¡rio dedicado, IP whitelisting, HTTPS |
| Custo de infraestrutura | Baixo | Baixa | Workers leves (200MB RAM), auto-scaling |
| Perda de mensagens | Alto | Baixa | PersistÃªncia RabbitMQ, confirmaÃ§Ã£o manual |
| Task duplicada | MÃ©dio | MÃ©dia | IdempotÃªncia, deduplicaÃ§Ã£o por ID |

---

## ğŸ’° Estimativa de Recursos

### Tempo
| Fase | Horas | Desenvolvedor |
|------|-------|---------------|
| Fase 1 | 4-6 | Backend |
| Fase 2 | 6-8 | Backend + Odoo |
| Fase 3 | 8-10 | Backend |
| Fase 4 | 6-8 | Fullstack |
| Fase 5 | 4-6 | Backend + DevOps |
| Fase 6 | 8-10 | QA + Backend |
| **Total** | **36-48 horas** | **~1-2 semanas** |

### Infraestrutura
| Componente | CPU | RAM | Storage |
|------------|-----|-----|---------|
| RabbitMQ | 0.5 core | 512 MB | 10 GB |
| Celery Worker | 1 core | 1 GB | 5 GB |
| Flower | 0.2 core | 256 MB | 1 GB |
| **Total** | **1.7 cores** | **1.8 GB** | **16 GB** |

---

## ğŸ“š ReferÃªncias

### DocumentaÃ§Ã£o Oficial
- [Celery Documentation](https://docs.celeryq.dev/en/stable/)
- [RabbitMQ Tutorials](https://www.rabbitmq.com/tutorials)
- [Flower Documentation](https://flower.readthedocs.io/)
- [Odoo XML-RPC API](https://www.odoo.com/documentation/18.0/developer/reference/external_api.html)

### Artigos e Tutoriais
- [Celery Best Practices](https://denibertovic.com/posts/celery-best-practices/)
- [RabbitMQ in 5 Minutes](https://www.cloudamqp.com/blog/part1-rabbitmq-for-beginners-what-is-rabbitmq.html)
- [Monitoring Celery with Flower](https://medium.com/@sanjaysingh/monitoring-celery-with-flower-7d1a2c6e3b5a)

### RepositÃ³rios Exemplo
- [Celery + Django](https://github.com/celery/celery/tree/main/examples/django)
- [Odoo External API](https://github.com/odoo/odoo/tree/18.0/doc/external_api)

---

## ğŸ“Œ Checklist Final

Antes de considerar a implementaÃ§Ã£o completa:

- [ ] Todos os containers sobem sem erros
- [ ] RabbitMQ Management acessÃ­vel
- [ ] Flower acessÃ­vel e mostrando workers
- [ ] Tasks aparecem no Flower ao serem enfileiradas
- [ ] Tasks sÃ£o executadas com sucesso
- [ ] Resultados retornam para o Odoo
- [ ] Cron job atualiza status corretamente
- [ ] Retry funciona em caso de falha
- [ ] NotificaÃ§Ãµes chegam ao usuÃ¡rio
- [ ] Logs estruturados e legÃ­veis
- [ ] Testes E2E passando
- [ ] DocumentaÃ§Ã£o completa
- [ ] ADR publicado

---

## ğŸ¯ PrÃ³ximos Passos (ApÃ³s ImplementaÃ§Ã£o)

### Melhorias Futuras
1. **PriorizaÃ§Ã£o de Tasks**
   - Filas separadas por prioridade (high, normal, low)
   - Dedicar workers para cada fila

2. **Scheduled Tasks**
   - Tasks agendadas (ETA)
   - Tasks recorrentes (periodic tasks)

3. **Monitoramento AvanÃ§ado**
   - IntegraÃ§Ã£o com Grafana/Prometheus
   - Alertas via Slack/Email
   - SLA tracking

4. **Scaling**
   - Auto-scaling de workers
   - Load balancing
   - Multi-region deployment

5. **SeguranÃ§a**
   - Criptografia de mensagens
   - AutenticaÃ§Ã£o mÃºtua
   - Rate limiting

---

**Status:** ğŸ“‹ Planejado (nÃ£o iniciado)  
**Prioridade:** ğŸŸ¢ Baixa  
**Implementar quando:** Houver necessidade real de processamento assÃ­ncrono pesado (> 100 tasks/dia)

**Criado em:** 2025-12-05  
**Ãšltima atualizaÃ§Ã£o:** 2025-12-05
